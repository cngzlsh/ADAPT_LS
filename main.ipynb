{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c293036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/daniel/.local/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "from CWIs.complex_labeller import Complexity_labeller\n",
    "# from plainifier.plainify import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9d16d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/daniel/Documents/MSc_ML_UCL/COMP0087/LSBert_new/CWIs/labeler.py:153: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/daniel/.local/lib/python3.9/site-packages/tensorflow/python/ops/rnn.py:437: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/daniel/.local/lib/python3.9/site-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:992: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "INFO:tensorflow:Converted call: <bound method Dense.call of <keras.legacy_tf_layers.core.Dense object at 0x7fbcde577e20>>\n",
      "    args: (<tf.Tensor 'chars/Reshape_2:0' shape=(?, ?, 200) dtype=float32>,)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Converted call: <bound method Dense.call of <keras.legacy_tf_layers.core.Dense object at 0x7fbcdd5af430>>\n",
      "    args: (<tf.Tensor 'chars/concat_1:0' shape=(?, ?, 600) dtype=float32>,)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Converted call: <bound method Dense.call of <keras.legacy_tf_layers.core.Dense object at 0x7fbcdd5af9a0>>\n",
      "    args: (<tf.Tensor 'chars/dense_1/Tanh:0' shape=(?, ?, 300) dtype=float32>,)\n",
      "    kwargs: {}\n",
      "\n",
      "WARNING:tensorflow:From /home/daniel/.local/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1082: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/Documents/MSc_ML_UCL/COMP0087/LSBert_new/CWIs/labeler.py:142: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  char_lstm_cell_fw = tf.nn.rnn_cell.LSTMCell(self.config[\"char_recurrent_size\"],\n",
      "/home/daniel/Documents/MSc_ML_UCL/COMP0087/LSBert_new/CWIs/labeler.py:147: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  char_lstm_cell_bw = tf.nn.rnn_cell.LSTMCell(self.config[\"char_recurrent_size\"],\n",
      "/home/daniel/.local/lib/python3.9/site-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:984: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  self._kernel = self.add_variable(\n",
      "/home/daniel/.local/lib/python3.9/site-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:993: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  self._bias = self.add_variable(\n",
      "/home/daniel/Documents/MSc_ML_UCL/COMP0087/LSBert_new/CWIs/labeler.py:166: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  char_output_tensor = tf.layers.dense(char_output_tensor, char_hidden_layer_size, activation=tf.tanh, kernel_initializer=self.initializer)\n",
      "/home/daniel/.local/lib/python3.9/site-packages/keras/legacy_tf_layers/core.py:261: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  return layer.apply(inputs)\n",
      "/home/daniel/Documents/MSc_ML_UCL/COMP0087/LSBert_new/CWIs/labeler.py:183: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  attention_output = tf.layers.dense(attention_evidence_tensor, self.config[\"word_embedding_size\"], activation=tf.tanh, kernel_initializer=self.initializer)\n",
      "/home/daniel/Documents/MSc_ML_UCL/COMP0087/LSBert_new/CWIs/labeler.py:184: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  attention_output = tf.layers.dense(attention_output, self.config[\"word_embedding_size\"], activation=tf.sigmoid, kernel_initializer=self.initializer)\n",
      "/home/daniel/Documents/MSc_ML_UCL/COMP0087/LSBert_new/CWIs/labeler.py:194: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  word_lstm_cell_fw = tf.nn.rnn_cell.LSTMCell(self.config[\"word_recurrent_size\"],\n",
      "/home/daniel/Documents/MSc_ML_UCL/COMP0087/LSBert_new/CWIs/labeler.py:199: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  word_lstm_cell_bw = tf.nn.rnn_cell.LSTMCell(self.config[\"word_recurrent_size\"],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Converted call: <bound method Dense.call of <keras.legacy_tf_layers.core.Dense object at 0x7fbcdcb02520>>\n",
      "    args: (<tf.Tensor 'lmcost_lstm_separate/strided_slice_4:0' shape=(?, ?, 300) dtype=float32>,)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Converted call: <bound method Dense.call of <keras.legacy_tf_layers.core.Dense object at 0x7fbcdcb02970>>\n",
      "    args: (<tf.Tensor 'lmcost_lstm_separate/lmcost_lstm_separate_fw/dense/Tanh:0' shape=(?, ?, 50) dtype=float32>,)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Converted call: <bound method Dense.call of <keras.legacy_tf_layers.core.Dense object at 0x7fbcdcab0b50>>\n",
      "    args: (<tf.Tensor 'lmcost_lstm_separate/strided_slice_6:0' shape=(?, ?, 300) dtype=float32>,)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Converted call: <bound method Dense.call of <keras.legacy_tf_layers.core.Dense object at 0x7fbcdcab0b50>>\n",
      "    args: (<tf.Tensor 'lmcost_lstm_separate/lmcost_lstm_separate_bw/dense/Tanh:0' shape=(?, ?, 50) dtype=float32>,)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Converted call: <bound method Dense.call of <keras.legacy_tf_layers.core.Dense object at 0x7fbcdcbca790>>\n",
      "    args: (<tf.Tensor 'concat:0' shape=(?, ?, 600) dtype=float32>,)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Converted call: <bound method Dense.call of <keras.legacy_tf_layers.core.Dense object at 0x7fbcdcb3a0d0>>\n",
      "    args: (<tf.Tensor 'dense/Tanh:0' shape=(?, ?, 50) dtype=float32>,)\n",
      "    kwargs: {}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/Documents/MSc_ML_UCL/COMP0087/LSBert_new/CWIs/labeler.py:264: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  lmcost_hidden_layer = tf.layers.dense(input_tensor, self.config[\"lmcost_hidden_layer_size\"], activation=tf.tanh, kernel_initializer=self.initializer)\n",
      "/home/daniel/Documents/MSc_ML_UCL/COMP0087/LSBert_new/CWIs/labeler.py:265: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  lmcost_output = tf.layers.dense(lmcost_hidden_layer, lmcost_max_vocab_size, activation=None, kernel_initializer=self.initializer)\n",
      "/home/daniel/Documents/MSc_ML_UCL/COMP0087/LSBert_new/CWIs/labeler.py:221: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  processed_tensor = tf.layers.dense(processed_tensor, self.config[\"hidden_layer_size\"], activation=tf.tanh, kernel_initializer=self.initializer)\n",
      "/home/daniel/Documents/MSc_ML_UCL/COMP0087/LSBert_new/CWIs/labeler.py:224: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  self.scores = tf.layers.dense(processed_tensor, len(self.label2id), activation=None, kernel_initializer=self.initializer, name=\"output_ff\")\n",
      "2022-03-30 22:41:35.470672: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-30 22:41:35.471397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-30 22:41:35.475712: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-03-30 22:41:35.475723: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "two_gram_mwes_list = './CWIs/2_gram_mwe_50.txt'\n",
    "three_gram_mwes_list = './CWIs/3_gram_mwe_25.txt'\n",
    "four_gram_mwes_list = './CWIs/4_gram_mwe_8.txt'\n",
    "pretrained_model_path = './CWIs/cwi_seq.model'\n",
    "temp_path = './CWIs/temp_file.txt'\n",
    "\n",
    "path = './plainifier/'\n",
    "premodel = 'bert-large-uncased-whole-word-masking'\n",
    "bert_dict = 'tersebert_pytorch_1_0.bin'\n",
    "embedding = 'crawl-300d-2M-subword.vec'\n",
    "unigram = 'unigrams-df.tsv'\n",
    "tokenizer = BertTokenizer.from_pretrained(premodel)\n",
    "Complexity_labeller_model = Complexity_labeller(pretrained_model_path, temp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19f06e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 2000000/2000000 [01:29<00:00, 22325.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Embeddings\n",
      "Loading Unigrams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 8394369/8394369 [05:55<00:00, 23583.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Unigrams\n"
     ]
    }
   ],
   "source": [
    "model, similm, tokenfreq, embeddings, vocabulary2 = load_all(path, premodel, bert_dict, embedding, unigram, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5d229d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexSentence:\n",
    "    # Sentence class\n",
    "    def __init__(self, sentence, label_model, tokeniser, verbose=True):\n",
    "        self.sentence = sentence\n",
    "        self.tokenised_sentence = tokeniser.tokenize(self.sentence)\n",
    "        self.label_model = label_model\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f'\\t Untokenised sentence: {self.sentence}')\n",
    "            print(f'\\t Tokenised sentence: {self.tokenised_sentence}')\n",
    "\n",
    "        self.label_complex_words()\n",
    "\n",
    "    def label_complex_words(self):\n",
    "        # applying complexity labeller to the sentence\n",
    "\n",
    "        Complexity_labeller.convert_format_string(self.label_model, self.sentence)\n",
    "        self.bin_labels = Complexity_labeller.get_bin_labels(self.label_model)\n",
    "        self.is_complex = True if np.sum(self.bin_labels) >= 1 else False\n",
    "        self.probs = Complexity_labeller.get_prob_labels(self.label_model)\n",
    "        \n",
    "        self.complexity_ranking = np.argsort(self.probs)[::-1]\n",
    "        self.most_complex_word = self.tokenised_sentence[self.complexity_ranking[0]]\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f'\\t Complex probs: {self.probs}')\n",
    "            print(f'\\t Binary complexity labels: {self.bin_labels}')\n",
    "            \n",
    "            if self.is_complex:\n",
    "                print(f'\\t Most complex word: {self.most_complex_word}')\n",
    "        \n",
    "            if not self.is_complex:\n",
    "                print(f'Simplificaiton complete')\n",
    "    \n",
    "    def find_MWEs_w_most_complex_word(self, n_gram, filepath):\n",
    "        # finds the n-gram mwe of the most complex word in the sentence, if any\n",
    "        # returns: mwe\n",
    "\n",
    "        complex_word_pos = self.complexity_ranking[0]\n",
    "        has_mwe = False\n",
    "\n",
    "        if complex_word_pos - n_gram + 1 > 0:\n",
    "            sliding_start = complex_word_pos - n_gram + 1\n",
    "        else:\n",
    "            sliding_start = 0\n",
    "        \n",
    "        if complex_word_pos + n_gram - 1 < len(self.complexity_ranking):\n",
    "            sliding_end = complex_word_pos\n",
    "        else:\n",
    "            sliding_end = len(self.complexity_ranking) - n_gram\n",
    "\n",
    "        with open(filepath, 'r') as f:\n",
    "            mwes = set(f.read().split('\\n')) # make set\n",
    "\n",
    "            for pos in range(sliding_start, sliding_end + 1):\n",
    "                possible_mwe = ' '.join(self.tokenised_sentence[pos: pos + n_gram])\n",
    "\n",
    "                if possible_mwe not in mwes:\n",
    "                    if self.verbose:\n",
    "                        print(f'\\t Checking MWE \"{possible_mwe}\": not in list')\n",
    "                else:\n",
    "                    valid_mwes_idx = np.arange(pos, pos+n_gram, 1)\n",
    "                    has_mwe = True\n",
    "                    if self.verbose:\n",
    "                        print(f'\\t Checking MWE \"{possible_mwe}\": MWE found!')\n",
    "        \n",
    "        if has_mwe:\n",
    "            self.to_display = possible_mwe\n",
    "        else:\n",
    "            self.to_display = self.most_complex_word\n",
    "        \n",
    "        if has_mwe:\n",
    "            return valid_mwes_idx\n",
    "        else:\n",
    "            return [complex_word_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "74903771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import copy\n",
    "\n",
    "from plainifier.planify_utils import *\n",
    "\n",
    "# Should GPU be used\n",
    "usingGPU=torch.cuda.is_available()\n",
    "\n",
    "# Global options\n",
    "\n",
    "def load_all(path, premodel, bert_dict, embedding, unigram, tokenizer):\n",
    "\n",
    "    # Load BERT model\n",
    "    model_dict = torch.load(path + bert_dict)\n",
    "    model = BertForMaskedLM.from_pretrained(pretrained_model_name_or_path=premodel, state_dict=model_dict)\n",
    "    vocabulary=tokenizer.get_vocab()\n",
    "    vocabulary2=['']*len(vocabulary)\n",
    "    for s in vocabulary:\n",
    "        vocabulary2[vocabulary[s]]=s\n",
    "\n",
    "    print(\"Loading Embeddings\")\n",
    "    # FastText embeddings, available from https://fasttext.cc/docs/en/english-vectors.html\n",
    "    pathW2V=path + embedding\n",
    "    embeddingsW = readWord2Vec(pathW2V)\n",
    "    embeddings = getToken2Vec(embeddingsW, tokenizer)\n",
    "    #np.save('./emb.npy',embeddings)\n",
    "    #embeddings=np.load('./emb.npy')\n",
    "    similm = np.matmul(embeddings,np.transpose(embeddings))\n",
    "    print(\"Loaded Embeddings\")\n",
    "\n",
    "    print(\"Loading Unigrams\")\n",
    "    pathUni1T = path + unigram\n",
    "    unifreq = readUnigramFreq(pathUni1T)\n",
    "    tokenfreq = getTokenFreq(unifreq, tokenizer)\n",
    "    #np.save('./freq.npy',tokenfreq)\n",
    "    #tokenfreq=np.load('./freq.npy')\n",
    "    print(\"Loaded Unigrams\")\n",
    "\n",
    "    return model, similm, tokenfreq, embeddings, vocabulary2\n",
    "\n",
    "# model, similm, tokenfreq, embeddings, vocabulary2 = load_all(path, premodel, bert_dict, embedding, unigram, tokenizer)\n",
    "\n",
    "# Helper functions\t\n",
    "\n",
    "# Tokenise, but keep the word information\n",
    "def tokeniseUntokenise(sentence, tokenizer):\n",
    "    tokenised_text = tokenizer.tokenize(sentence)\n",
    "    is_word=[not str.startswith(\"##\") for str in tokenised_text]\n",
    "    words=[]\n",
    "    currentWord=[]\n",
    "    for i in range(len(is_word)):\n",
    "        if is_word[i] and currentWord!=[]:\n",
    "            words.append(currentWord)\n",
    "            currentWord=[]\n",
    "        currentWord.append(i)\n",
    "    words.append(currentWord)\n",
    "    result={}\n",
    "    result['tokens']=tokenised_text\n",
    "    result['words']=words\n",
    "    return(result)\n",
    "\n",
    "# Mask the tokens selected through dummies ('_')\n",
    "def maskDummies(tokenisedOriginal,tokenisedDummies,prefixOriginal,addPAD=0):\n",
    "    if prefixOriginal:\n",
    "        maskedTokens=['[CLS]']+tokenisedOriginal['tokens']+['[SEP]']+tokenisedDummies['tokens']+['[SEP]']\n",
    "        offset=1+len(tokenisedOriginal['tokens'])\n",
    "    else:\n",
    "        maskedTokens=['[CLS]']+tokenisedDummies['tokens']+['[SEP]']\n",
    "        offset=0\n",
    "    for i in range(len(tokenisedDummies['tokens'])):\n",
    "        if tokenisedDummies['tokens'][i]=='_':\n",
    "            maskedTokens[i+1+offset]='[MASK]'\n",
    "    maskedTokens.extend(['[PAD]']*addPAD)\n",
    "    return(maskedTokens)\n",
    "\n",
    "# Prepare input vectors for masked sentences\n",
    "def prepareVectors(masked, prefixOriginal, tokenizer):\n",
    "    if not prefixOriginal:\n",
    "        return (([tokenizer.convert_tokens_to_ids(x) for x in masked],[[0]*len(x) for x in masked]))\n",
    "    indexed=[]\n",
    "    segments=[]\n",
    "    for i in range(len(masked)):\n",
    "        indexedI=tokenizer.convert_tokens_to_ids(masked[i])\n",
    "        segmentsI=[]\n",
    "        currentSegment=0\n",
    "        for j in range(len(masked[i])):\n",
    "            segmentsI.append(currentSegment)\n",
    "            if masked[i][j]=='[SEP]':\n",
    "                currentSegment=1\n",
    "        indexed.append(indexedI)\n",
    "        segments.append(segmentsI)\n",
    "    return ((indexed,segments))\n",
    "\n",
    "# Get predictions from BERT\n",
    "def getPredictions(indexed,segments,prefixOriginal,onGPU,model):\n",
    "    tokens_tensor = torch.tensor(indexed)\n",
    "    segments_tensors = torch.tensor(segments)\n",
    "    if onGPU:\n",
    "        tokens_tensor = tokens_tensor.to('cuda')\n",
    "        segments_tensors = segments_tensors.to('cuda')\n",
    "        model.to('cuda')\n",
    "    with torch.no_grad():\n",
    "        predictions = model(tokens_tensor, segments_tensors)\n",
    "    if onGPU:\n",
    "        predictions=(predictions[0].cpu(),)\n",
    "    if prefixOriginal:\n",
    "        offset=sum(np.array(segments[0])==0)-1\n",
    "        predictions=(predictions[0][:,offset:,:],)\n",
    "    return(predictions)\n",
    "\n",
    "# Replace a word at a given location with a given token\n",
    "def replaceWord(tokenised, word, replacement):\n",
    "    result={'tokens':[],'words':[]}\n",
    "    offset=0\n",
    "    for i in range(len(tokenised['words'])):\n",
    "        if i==word:\n",
    "            if replacement is None:\n",
    "                offset=len(tokenised['words'][i])\n",
    "            else:\n",
    "                result['tokens'].append(replacement)\n",
    "                result['words'].append([tokenised['words'][i][0]])\n",
    "                offset=len(tokenised['words'][i])-1\n",
    "        else:\n",
    "            result['tokens'].extend([tokenised['tokens'][x] for x in tokenised['words'][i]])\t\t\n",
    "            result['words'].append([x-offset for x in tokenised['words'][i]])\n",
    "    return(result)\n",
    "\n",
    "# Insert a dummy just before a given position\n",
    "def insertDummy(tokenised,word):\n",
    "    result={'tokens':[],'words':[]}\n",
    "    offset=0\n",
    "    for i in range(len(tokenised['words'])):\n",
    "        if i==word:\n",
    "            result['tokens'].append('_')\n",
    "            result['words'].append([tokenised['words'][i][0]])\n",
    "            offset=1\n",
    "        result['tokens'].extend([tokenised['tokens'][x] for x in tokenised['words'][i]])\t\t\n",
    "        result['words'].append([x+offset for x in tokenised['words'][i]])\n",
    "    if word==len(tokenised['words']):\n",
    "        # Add dummy at the end\n",
    "        result['tokens'].append('_')\n",
    "        result['words'].append([len(tokenised['tokens'])])\n",
    "    return(result)\n",
    "\n",
    "\n",
    "# Convert tokens to words\n",
    "def getWords2(tokens):\n",
    "    if tokens==[]:\n",
    "        return(tokens)\n",
    "    words=[]\n",
    "    for token in tokens:\n",
    "        if words==[]:\n",
    "            words.append(token)\n",
    "        elif not token.startswith(\"##\"):\n",
    "            words.append(token)\n",
    "        else:\n",
    "            words[len(words)-1]=words[len(words)-1]+token.replace(\"##\",\"\")\n",
    "    return(words)\n",
    "\n",
    "# Get similarity between two sets of token identifiers by aligning them according to similarity\n",
    "def getSimilarityAligned(oldTids,newTids,similm):\n",
    "    subM=similm[oldTids,]\n",
    "    part1=np.amax(subM,0)\n",
    "    subM=np.copy(subM)\n",
    "    if len(newTids)>0:\n",
    "        for i in range(len(oldTids)):\n",
    "            best=np.max(subM[i,newTids])\n",
    "            subM[i,subM[i,]<best]=best\n",
    "    result=(np.sum(subM,0)+part1+np.sum(part1[newTids]))/(len(oldTids)+len(newTids)+1)\n",
    "    return(result)\n",
    "\n",
    "def getSimilarityAveraged(oldTids,newTids,embeddings):\n",
    "    oldVector=np.sum(embeddings[oldTids],axis=0)/(len(oldTids))\n",
    "    oldVector=oldVector/np.sqrt(np.sum(oldVector*oldVector))\n",
    "    newVectors=np.sum(embeddings[newTids],axis=0)\n",
    "    mat=embeddings+newVectors\n",
    "    mat=mat/np.sqrt(np.sum(mat*mat,axis=1))[:,np.newaxis]\n",
    "    result=np.matmul(mat,oldVector)\n",
    "    return(result)\n",
    "\n",
    "# Obtain BERT predictions in a current context\n",
    "def replacementPredictions(tokenised,currentTokenised,mweStart,prefixOriginal,model,usingGPU,maxDepth,tokenizer):\n",
    "    # Prepare prediction task for single mask\n",
    "    withdummies=insertDummy(currentTokenised,mweStart)\n",
    "    masked1=maskDummies(tokenised,withdummies,prefixOriginal,1)\n",
    "    if maxDepth>1:\n",
    "        # and double mask\n",
    "        withdummies=insertDummy(withdummies,mweStart)\n",
    "        masked2=maskDummies(tokenised,withdummies,prefixOriginal,0)\n",
    "        masked=[masked1,masked2]\n",
    "    else:\n",
    "        masked=[masked1]\n",
    "    indexed,segments=prepareVectors(masked,prefixOriginal,tokenizer)\n",
    "    predictions=getPredictions(indexed,segments,prefixOriginal,usingGPU,model)\n",
    "    return(predictions)\n",
    "\n",
    "# Obtain BERT predictions in a current context\n",
    "def multiReplacementPredictions(tokenised,currentTokenised,mweStart,prefixOriginal,model,usingGPU,maxDepth,tokenizer):\n",
    "    masked=[]\n",
    "    for key in currentTokenised:\n",
    "        # Prepare prediction task for single mask\n",
    "        withdummies=insertDummy(currentTokenised[key],mweStart)\n",
    "        masked1=maskDummies(tokenised,withdummies,prefixOriginal,1)\n",
    "        masked.append(masked1)\n",
    "        if maxDepth>1:\n",
    "            # and double mask\n",
    "            withdummies=insertDummy(withdummies,mweStart)\n",
    "            masked2=maskDummies(tokenised,withdummies,prefixOriginal,0)\n",
    "            masked.append(masked2)\n",
    "    indexed,segments=prepareVectors(masked,prefixOriginal,tokenizer)\n",
    "    predictions=getPredictions(indexed,segments,prefixOriginal,usingGPU,model)\n",
    "    result={}\n",
    "    i=0\n",
    "    for key in currentTokenised:\n",
    "        if maxDepth>1:\n",
    "            result[key]=(predictions[0][i:(i+2)],)\n",
    "            i=i+2\n",
    "        else:\n",
    "            result[key]=(predictions[0][i:(i+1)],)\n",
    "            i=i+1\n",
    "    return(result)\n",
    "\n",
    "# Compute how good the current fragment is for being a replacement\n",
    "def getGoodness(current,origTids,scores,tokenfreq,similm,tokenizer,alpha=(1,1,1)):\n",
    "    # Probability of previous tokens times that of candidates\n",
    "    scoresVec=scores*current['score']\n",
    "    # Frequency of candidates, unless higher than of previous tokens\n",
    "    freqVec=np.copy(tokenfreq)\n",
    "    freqVec[freqVec>current['freqs']]=current['freqs']\n",
    "    # Similarity taking into account previous tokens\n",
    "    #similVec=getSimilarityAveraged(origTids,tokenizer.convert_tokens_to_ids(current['tokens']))\n",
    "    similVec=getSimilarityAligned(origTids,tokenizer.convert_tokens_to_ids(current['tokens']),similm)\n",
    "    finalGoodness=(scoresVec**alpha[0])*(similVec**alpha[1])*(freqVec**alpha[2])\n",
    "    return (np.transpose(np.array([finalGoodness,scoresVec,similVec,freqVec])))\n",
    "\n",
    "# Aggregate results from two sources, i.e. running Plainifier forwards and backwards\n",
    "def aggregateResults(results):\n",
    "    allWordsList=[]\n",
    "    allWordsSet=set()\n",
    "    allScores=None\n",
    "    for words,scores in results:\n",
    "        if allWordsList==[]:\n",
    "            allWordsList=words.copy()\n",
    "            allWordsSet.update(words)\n",
    "            allScores=scores\n",
    "        else:\n",
    "            newWordsI=[not(word in allWordsSet) for word in words]\n",
    "            newWords=[word for word in words if not(word in allWordsSet)]\n",
    "            allWordsList.extend(newWords)\n",
    "            allWordsSet.update(newWords)\n",
    "            allScores=np.concatenate((allScores,scores[newWordsI,]))\n",
    "    sortedTop=((-allScores[:,0]).argsort())\n",
    "    return(([allWordsList[i] for i in sortedTop],allScores[sortedTop,]))\n",
    "\n",
    "# Get replacements for the specified words\n",
    "def getTokenReplacement(tokenised, mweStart, mweLength, tokenizer, model, similm, tokenfreq, embeddings, vocabulary2, current=None, verbose=False, backwards=False, none_threshold=0.5, maxDepth=3, maxBreadth=16, alpha=(1,1,1)):\n",
    "\n",
    "    prefixOriginal = True\n",
    "    # If no depth left, return empty\n",
    "    if maxDepth==0:\n",
    "        return (([],[]))\n",
    "    # Handle root call\n",
    "    head=False\n",
    "    if current is None:\n",
    "        head=True\n",
    "        current={}\n",
    "        current['tokenised']=copy.deepcopy(tokenised)\n",
    "        # Remove the target words\n",
    "        for i in range(mweLength):\n",
    "            current['tokenised']=replaceWord(current['tokenised'],mweStart,None)\n",
    "        current['tokens']=[]\n",
    "        current['score']=1.0\n",
    "        current['freqs']=1.0\n",
    "        current['origTids']=tokenizer.convert_tokens_to_ids(tokenised['tokens'][(tokenised['words'][mweStart][0]):(tokenised['words'][mweStart+mweLength][0])])\n",
    "        current['guaranteedTids']=current['origTids']\n",
    "        \n",
    "    # Run the predictions, unless precomputed above\n",
    "    if 'predictions' in current:\n",
    "        predictions=current['predictions']\n",
    "    else:\n",
    "        predictions=replacementPredictions(tokenised,current['tokenised'],mweStart,prefixOriginal,model,usingGPU,maxDepth,tokenizer)\n",
    "    \n",
    "    # Handle single mask results\n",
    "    scores=predictions[0][0][current['tokenised']['words'][mweStart][0]+1].numpy()\n",
    "    scores=np.exp(scores)/sum(np.exp(scores))\n",
    "    none_score=scores[tokenizer.convert_tokens_to_ids(\"[unused0]\")]\n",
    "    if verbose:\n",
    "        print((\"\\t\"*(3-maxDepth))+\"NONE: \"+str(none_score))\n",
    "    # If none threshold exceeded, return empty\n",
    "    if none_score>none_threshold and not head:\n",
    "        maxDepth=0\n",
    "    # Compute goodness\n",
    "    goodness=getGoodness(current,current['origTids'],scores,tokenfreq,similm,tokenizer,alpha=alpha)\n",
    "    \n",
    "    if backwards:\n",
    "        resultTokens=[[vocabulary2[i]]+current['tokens'] for i in range(len(vocabulary2))[999:]]\n",
    "    else:\n",
    "        resultTokens=[current['tokens']+[vocabulary2[i]] for i in range(len(vocabulary2))[999:]]\n",
    "    resultGoodness=goodness[999:,]\n",
    "    if verbose:\n",
    "        for i in (-goodness[:,0]).argsort()[:5]:\n",
    "            print((\"\\t\"*(3-maxDepth))+vocabulary2[i]+\": \"+str(resultGoodness[i,0]))\n",
    "    if maxDepth>1 and maxBreadth>0:\n",
    "        # Get double-word replacements\n",
    "        if backwards:\n",
    "            scores=predictions[0][1][current['tokenised']['words'][mweStart][0]+2].numpy()\n",
    "        else:\n",
    "            scores=predictions[0][1][current['tokenised']['words'][mweStart][0]+1].numpy()\n",
    "        scores=np.exp(scores)/sum(np.exp(scores))\n",
    "        # Compute goodness\n",
    "        goodness=getGoodness(current,current['origTids'],scores,tokenfreq,similm,tokenizer,alpha=alpha)\n",
    "        \n",
    "        # Choose the path\n",
    "        sortedTop=(-goodness[:,0]).argsort()[0:maxBreadth]\n",
    "        if len(current['guaranteedTids'])>0:\n",
    "            if backwards:\n",
    "                guaranteedTid=current['guaranteedTids'][-1]\n",
    "                current['guaranteedTids']=current['guaranteedTids'][:-1]\n",
    "            else:\n",
    "                guaranteedTid=current['guaranteedTids'][0]\n",
    "                current['guaranteedTids']=current['guaranteedTids'][1:]\n",
    "            if not (guaranteedTid in sortedTop):\n",
    "                scores[guaranteedTid]=scores[sortedTop[-1]]\n",
    "                sortedTop[-1]=guaranteedTid\n",
    "            \n",
    "        # Prepare data for recursive execution\n",
    "        currentNews={}\n",
    "        for i in sortedTop:\n",
    "            bestCandidate=vocabulary2[i]\n",
    "            if bestCandidate==\"[unused0]\":\n",
    "                continue\n",
    "            newsentence=insertDummy(current['tokenised'],mweStart)\n",
    "            newsentence=replaceWord(newsentence,mweStart,bestCandidate)\n",
    "            currentNew={}\n",
    "            currentNew['tokenised']=newsentence\n",
    "            currentNew['origTids']=current['origTids']\n",
    "            currentNew['guaranteedTids']=current['guaranteedTids']\n",
    "            if backwards:\n",
    "                currentNew['tokens']=[bestCandidate]+current['tokens']\n",
    "            else:\n",
    "                currentNew['tokens']=current['tokens']+[bestCandidate]\n",
    "            currentNew['score']=current['score']*scores[i]\n",
    "            currentNew['freqs']=min(current['freqs'],tokenfreq[i])\n",
    "            currentNews[i]=currentNew\n",
    "            \n",
    "        \n",
    "        \n",
    "        # Pre-compute predictions\n",
    "        newsentences={i: currentNews[i]['tokenised'] for i in sortedTop}\n",
    "        if backwards:\n",
    "            newmweStart=mweStart\n",
    "        else:\n",
    "            newmweStart=mweStart+1\n",
    "        preds=multiReplacementPredictions(tokenised,newsentences,newmweStart,prefixOriginal,model,usingGPU,maxDepth-1,tokenizer)\n",
    "        counter=0\n",
    "        # Run recursively\n",
    "        \n",
    "        for i in sortedTop:\n",
    "            bestCandidate=vocabulary2[i]\n",
    "            if bestCandidate==\"[unused0]\":\n",
    "                continue\n",
    "            if verbose:\n",
    "                print((\"\\t\"*(3-maxDepth))+str(counter)+\"/\"+str(maxBreadth)+\" \"+bestCandidate)\n",
    "            \n",
    "            counter=counter+1\n",
    "            currentNews[i]['predictions']=preds[i]\n",
    "            nextTokens,nextGoodness=getTokenReplacement(processed_sentence,\n",
    "                                                        newmweStart,\n",
    "                                                        0,\n",
    "                                                        tokenizer,\n",
    "                                                        model, \n",
    "                                                        similm, \n",
    "                                                        tokenfreq, \n",
    "                                                        embeddings, \n",
    "                                                        vocabulary2, \n",
    "                                                        current=currentNews[i],\n",
    "                                                        verbose=verbose,\n",
    "                                                        backwards=backwards,\n",
    "                                                        none_threshold=none_threshold,\n",
    "                                                        maxDepth=maxDepth-1,\n",
    "                                                        maxBreadth=round(maxBreadth/2),\n",
    "                                                        alpha=alpha)\n",
    "            if len(nextTokens)==0:\n",
    "                continue\n",
    "            \n",
    "            resultTokens=resultTokens+nextTokens\n",
    "            resultGoodness=np.concatenate((resultGoodness,nextGoodness))\n",
    "    # If not head call, just return candidates\n",
    "    if not head:\n",
    "        return((resultTokens,resultGoodness))\n",
    "    # Otherwise, rank the candidates\n",
    "    sortedTop=(-(resultGoodness[:,0])).argsort()\n",
    "    if verbose:\n",
    "        for i in range(50):\n",
    "            print(str(resultGoodness[sortedTop[i]])+\" : \"+str(resultTokens[sortedTop[i]]))\n",
    "    resultGoodness=resultGoodness[sortedTop,:]\n",
    "\n",
    "    resultTokens=[\" \".join(getWords2(resultTokens[i])) for i in sortedTop]\n",
    "    result=(resultTokens,resultGoodness)\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7862262c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_sentence(input_sentence):\n",
    "    s = ComplexSentence(input_sentence, label_model=Complexity_labeller_model, tokeniser=tokenizer, verbose=False)\n",
    "    complex_word_idx = s.find_MWEs_w_most_complex_word(n_gram=3, filepath=three_gram_mwes_list)\n",
    "    print(f'Your input sentence is: {input_sentence}')\n",
    "    print(f'Complex MWE or word found: {s.to_display}')\n",
    "    processed_sentence = tokeniseUntokenise(input_sentence, tokenizer)\n",
    "    result1 = getTokenReplacement(processed_sentence, \n",
    "                                  complex_word_idx[0], \n",
    "                                  len(complex_word_idx), \n",
    "                                  tokenizer,\n",
    "                                  model, \n",
    "                                  similm, \n",
    "                                  tokenfreq, \n",
    "                                  embeddings, \n",
    "                                  vocabulary2, \n",
    "                                  verbose=False, \n",
    "                                  backwards=False, \n",
    "                                  maxDepth=3, \n",
    "                                  maxBreadth=16, \n",
    "                                  alpha=(1/9,6/9,2/9))\n",
    "\n",
    "    result2 = getTokenReplacement(processed_sentence,\n",
    "                                  complex_word_idx[0],\n",
    "                                  len(complex_word_idx),\n",
    "                                  tokenizer,\n",
    "                                  model, \n",
    "                                  similm, \n",
    "                                  tokenfreq, \n",
    "                                  embeddings, \n",
    "                                  vocabulary2,\n",
    "                                  verbose=False,\n",
    "                                  backwards=True,\n",
    "                                  maxDepth=3,\n",
    "                                  maxBreadth=16,\n",
    "                                  alpha=(1/9,6/9,2/9))\n",
    "\n",
    "    words, scores = aggregateResults((result1,result2))\n",
    "    print(f'Suggested top 5 subtitutions: {words[:5]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "476bcdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your input sentence is: Katie takes a sip of her cappuccino\n",
      "Complex MWE or word found: cap\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_138509/4168842722.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimplify_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Katie takes a sip of her cappuccino\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_138509/2830841651.py\u001b[0m in \u001b[0;36msimplify_sentence\u001b[0;34m(input_sentence)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Complex MWE or word found: {s.to_display}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprocessed_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokeniseUntokenise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     result1 = getTokenReplacement(processed_sentence, \n\u001b[0m\u001b[1;32m      8\u001b[0m                                   \u001b[0mcomplex_word_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                                   \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomplex_word_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_138509/1337332998.py\u001b[0m in \u001b[0;36mgetTokenReplacement\u001b[0;34m(tokenised, mweStart, mweLength, tokenizer, model, similm, tokenfreq, embeddings, vocabulary2, current, verbose, backwards, none_threshold, maxDepth, maxBreadth, alpha)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mcurrent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mcurrent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'freqs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mcurrent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'origTids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenised\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenised\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmweStart\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenised\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmweStart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmweLength\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0mcurrent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'guaranteedTids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'origTids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "simplify_sentence(\"Katie takes a sip of her cappuccino\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8362ce6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
