{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da3caa80",
   "metadata": {},
   "source": [
    "## Improving Lexical Simplification Using State of the Art Lexical Complexity Prediction Models\n",
    "#### Demo notebook\n",
    "This notebook provides a demonstration of simplifying sentence with multi-word expressions. It is tested on a system with the following specifications:\n",
    "<ol>\n",
    "    <li>OS: Linux x86-64</li>\n",
    "    <li>CPU: 3.30 Ghz x 8</li>\n",
    "    <li>RAM: 40 GiB</li>\n",
    "    <li>Hard Drive: 20 GiB</li>\n",
    "    <li>GPU: NVIDIA Corporation GA104M (CUDA compute capability: 8.6)</li>\n",
    " </ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6cddf7",
   "metadata": {},
   "source": [
    "#### Run the following two cells to import packages and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c293036",
   "metadata": {},
   "outputs": [],
   "source": [
    " # imports\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import codecs\n",
    "import nltk\n",
    "\n",
    "from CWIs.complex_labeller import Complexity_labeller\n",
    "from plainifier.plainify import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9d16d90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-04 14:39:52.903752: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-04 14:39:52.907072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-04 14:39:52.908017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-04 14:39:52.908127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-04 14:39:57.184144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-04 14:39:57.184366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-04 14:39:57.184516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-04 14:39:57.185192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7982 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# settings\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "two_gram_mwes_list = './CWIs/2_gram_mwe_50.txt'\n",
    "three_gram_mwes_list = './CWIs/3_gram_mwe_25.txt'\n",
    "four_gram_mwes_list = './CWIs/4_gram_mwe_8.txt'\n",
    "pretrained_model_path = './CWIs/cwi_seq.model'\n",
    "temp_path = './CWIs/temp_file.txt'\n",
    "\n",
    "path = './plainifier/'\n",
    "premodel = 'bert-large-uncased-whole-word-masking'\n",
    "bert_dict = 'tersebert_pytorch_1_0.bin'\n",
    "embedding = 'crawl-300d-2M-subword.vec'\n",
    "unigram = 'unigrams-df.tsv'\n",
    "tokenizer = BertTokenizer.from_pretrained(premodel)\n",
    "Complexity_labeller_model = Complexity_labeller(pretrained_model_path, temp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19f06e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 2000000/2000000 [01:31<00:00, 21897.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Embeddings\n",
      "Loading Unigrams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 8394369/8394369 [06:05<00:00, 22970.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Unigrams\n"
     ]
    }
   ],
   "source": [
    "# loading bert model, word embeddings and unigrams. This process takes 7 minutes\n",
    "model, similm, tokenfreq, embeddings, vocabulary2 = load_all(path, premodel, bert_dict, embedding, unigram, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e320c1",
   "metadata": {},
   "source": [
    "#### Run the following cell to construct the sentence class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "34422a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexSentence:\n",
    "    # Sentence class\n",
    "    def __init__(self, sentence, label_model, tokeniser, verbose=True, beam_width=3, alpha=(1/9, 6/9, 2/9)):\n",
    "        self.sentence = sentence\n",
    "        self.tokenised_sentence = self.generate_tokenised_sentence()\n",
    "        \n",
    "        self.label_model = label_model\n",
    "        self.verbose = verbose\n",
    "        self.beam_width = beam_width\n",
    "        self.alpha = alpha\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f'Untokenised sentence: {self.sentence}')\n",
    "            print(f'Tokenised sentence: {self.tokenised_sentence}')\n",
    "        \n",
    "        self.label_complex_words()\n",
    "    \n",
    "    def generate_NER_filter(self, init=True):\n",
    "        # Generate case sensitive tokens\n",
    "        case_sensitive_tokens = nltk.tokenize.word_tokenize(self.sentence)\n",
    "        pos_tags = nltk.pos_tag(case_sensitive_tokens)\n",
    "        \n",
    "        list_of_NERs = []\n",
    "        for x in pos_tags:\n",
    "            if x[1] == 'NNP':\n",
    "                list_of_NERs.append(x[0].lower())\n",
    "        \n",
    "        # NER mask: np array, 0 if is_NER, 1 if not\n",
    "        NER_mask = np.ones_like(self.tokenised_sentence, dtype=np.int64)\n",
    "        for i in range(len(self.tokenised_sentence)):\n",
    "            if self.tokenised_sentence[i] in list_of_NERs:\n",
    "                NER_mask[i] = 0\n",
    "        assert len(NER_mask) == len(self.tokenised_sentence)\n",
    "        \n",
    "        if self.verbose:\n",
    "            if init and len(list_of_NERs) > 0:\n",
    "                print('Found NERs:', list_of_NERs)\n",
    "            elif init and len(list_of_NERs) == 0:\n",
    "                print('No NER found.')\n",
    "        return NER_mask\n",
    "        \n",
    "    def generate_tokenised_sentence(self):\n",
    "        tokens = tokeniseUntokenise(self.sentence, tokenizer)['tokens']\n",
    "        word_idx = tokeniseUntokenise(self.sentence, tokenizer)['words']\n",
    "        tokenised_sentence_list = []\n",
    "        for idx_list in word_idx:\n",
    "            if len(idx_list)==1:\n",
    "                tokenised_sentence_list.append(np.array(tokens)[idx_list[0]])\n",
    "            else:\n",
    "                word_untokenised = ''\n",
    "                for idx_list_untokenised in idx_list:\n",
    "                    word_untokenised += np.array(tokens)[idx_list_untokenised].replace('##', '')\n",
    "                tokenised_sentence_list.append(word_untokenised)\n",
    "        return tokenised_sentence_list\n",
    "    \n",
    "    def known_complexity(self):\n",
    "        tokens = tokeniseUntokenise(self.sentence, tokenizer)['tokens']\n",
    "        word_idx = tokeniseUntokenise(self.sentence, tokenizer)['words']\n",
    "        known_index = []\n",
    "        for idx_list in word_idx:\n",
    "            if len(idx_list)==1 and not re.match(r'^[_\\W]+$', tokens[idx_list[0]]):\n",
    "                #If known label as True\n",
    "                known_index.append(True)\n",
    "            else:\n",
    "                #If unknown label as False\n",
    "                known_index.append(False)\n",
    "        return known_index\n",
    "    \n",
    "    def label_complex_words(self, init=True):\n",
    "        \n",
    "        # applying complexity labeller to the sentence\n",
    "        Complexity_labeller.convert_format_string(self.label_model, self.sentence)\n",
    "        if init:\n",
    "            self.bin_labels = Complexity_labeller.get_bin_labels(self.label_model)[0]\n",
    "        self.probs = Complexity_labeller.get_prob_labels(self.label_model)\n",
    "        \n",
    "        # apply known complexity and NER mask\n",
    "        self.bin_labels = np.multiply(self.bin_labels, self.known_complexity())\n",
    "        self.bin_labels = np.multiply(self.bin_labels, self.generate_NER_filter(init=init))\n",
    "        self.probs = np.multiply(self.probs, self.known_complexity())\n",
    "        self.probs = np.multiply(self.probs, self.generate_NER_filter(init=False))\n",
    "        \n",
    "        self.is_complex = True if np.sum(self.bin_labels) >= 1 else False\n",
    "\n",
    "        self.complexity_ranking = np.argsort(np.array(self.bin_labels) * np.array(self.probs))[::-1]\n",
    "        self.most_complex_word = self.tokenised_sentence[self.complexity_ranking[0]]\n",
    "\n",
    "        if self.verbose and init:\n",
    "            print(f'Complex probs: {self.probs}')\n",
    "            print(f'Binary complexity labels: {self.bin_labels}')\n",
    "\n",
    "        if self.is_complex:\n",
    "            print(f'\\t Most complex word: {self.most_complex_word} \\n')\n",
    "\n",
    "        if not self.is_complex:\n",
    "            print(f'\\t Simplificaiton complete or no complex expression found.\\n')\n",
    "    \n",
    "    def find_MWEs_w_most_complex_word(self, n_gram, filepath):\n",
    "        # finds the n-gram mwe of the most complex word in the sentence, if any\n",
    "        # returns: mwe positions or complex word positions\n",
    "        \n",
    "        complex_word_pos = self.complexity_ranking[0]\n",
    "\n",
    "        if complex_word_pos - n_gram + 1 > 0:\n",
    "            sliding_start = complex_word_pos - n_gram + 1\n",
    "        else:\n",
    "            sliding_start = 0\n",
    "        \n",
    "        if complex_word_pos + n_gram - 1 < len(self.complexity_ranking):\n",
    "            sliding_end = complex_word_pos\n",
    "        else:\n",
    "            sliding_end = len(self.complexity_ranking) - n_gram\n",
    "\n",
    "        with open(filepath, 'r') as f:\n",
    "            mwes = set(f.read().split('\\n')) # make set\n",
    "            avg_mwe_complexity = 0\n",
    "            for pos in range(sliding_start, sliding_end + 1):\n",
    "                possible_mwe = ' '.join(self.tokenised_sentence[pos: pos + n_gram])\n",
    "                \n",
    "                if possible_mwe in mwes:\n",
    "                    \n",
    "                    if np.mean(self.probs[pos:pos+n_gram]) > avg_mwe_complexity:\n",
    "                        avg_mwe_complexity = np.mean(self.probs[pos:pos+n_gram])\n",
    "                        valid_mwes_idx = np.arange(pos, pos+n_gram, 1)\n",
    "                        mwe_found = possible_mwe\n",
    "                    else:\n",
    "                        continue\n",
    "                        \n",
    "        if avg_mwe_complexity > 0:\n",
    "            self.idx_to_plainify = valid_mwes_idx\n",
    "        else:\n",
    "            self.idx_to_plainify = [complex_word_pos]\n",
    "        \n",
    "    \n",
    "    def find_all_ngram_mwes(self):\n",
    "        # returns: self.idx_to_plainify the indices of the longest mwe found\n",
    "        \n",
    "        if not self.is_complex:\n",
    "            raise ValueError('Sentence is not complex')\n",
    "        \n",
    "        # give priority to longer MWEs\n",
    "        n_gram_files = {2: two_gram_mwes_list, 3: three_gram_mwes_list, 4:four_gram_mwes_list}\n",
    "        \n",
    "        for n in reversed(range(2,5)):\n",
    "            self.find_MWEs_w_most_complex_word(n, n_gram_files[n])\n",
    "            \n",
    "            if len(self.idx_to_plainify) == n: # if such mwe is found\n",
    "                break\n",
    "    \n",
    "    def one_step_plainify(self):\n",
    "        idx_start = self.idx_to_plainify[0]\n",
    "        idx_end = self.idx_to_plainify[-1]+1\n",
    "        complex_word_name = \" \".join(self.tokenised_sentence[idx_start:idx_end])\n",
    "        print(f'Found complex word or expression: ### {complex_word_name} ###. Plainifying...')\n",
    "        processed_sentence = tokeniseUntokenise(self.sentence, tokenizer)\n",
    "        forward_result = getTokenReplacement(processed_sentence, idx_start, len(self.idx_to_plainify), \n",
    "                                  tokenizer, model, similm, tokenfreq, embeddings, vocabulary2,\n",
    "                                  verbose=False, backwards=False, maxDepth=3, maxBreadth=16, alpha=self.alpha)\n",
    "        backward_result = getTokenReplacement(processed_sentence, idx_start, len(self.idx_to_plainify),\n",
    "                                  tokenizer, model, similm, tokenfreq, embeddings, vocabulary2, \n",
    "                                  verbose=False, backwards=True, maxDepth=3, maxBreadth=16, alpha=self.alpha)\n",
    "        words, scores = aggregateResults((forward_result, backward_result))\n",
    "        words = [w.replace('#', '') for w in words]\n",
    "        print(f'Suggested top 5 subtitutions: {words[:5]}')\n",
    "        \n",
    "        return words[0].split(' ')\n",
    "        \n",
    "    \n",
    "    def sub_in_sentence(self, substitution):\n",
    "        # plugs a substitution in the sentence, then updates complexity scores\n",
    "        substitution_len = len(substitution)\n",
    "        \n",
    "        idx_start = self.idx_to_plainify[0]\n",
    "        idx_end = self.idx_to_plainify[-1]+1\n",
    "        \n",
    "        self.tokenised_sentence = self.tokenised_sentence[:idx_start] + substitution + self.tokenised_sentence[idx_end:]\n",
    "        self.sentence = ' '.join(self.tokenised_sentence)\n",
    "        self.bin_labels = list(self.bin_labels[:idx_start]) + [0] * substitution_len + list(self.bin_labels[idx_end:])\n",
    "        self.label_complex_words(init=False)\n",
    "        print(f'\\t Sentence after substitution: {self.sentence}\\n')\n",
    "        \n",
    "    def recursive_greedy_plainify(self, max_steps=float('inf'), test=False):\n",
    "        n = 1\n",
    "        sub_details_list = []\n",
    "        while self.is_complex and n <= max_steps:\n",
    "            self.find_all_ngram_mwes()\n",
    "            sub = self.one_step_plainify()\n",
    "            self.sub_in_sentence(sub)\n",
    "            #append subtitution details\n",
    "            sub_details = {\"iteration\":n,\"sub_word\":sub[0],\"idx_start\":self.idx_to_plainify[0],\"idx_end\":self.idx_to_plainify[-1]+1}\n",
    "            sub_details_list.append(sub_details)\n",
    "            n += 1\n",
    "        print(f'Simplification complete.')\n",
    "        \n",
    "        if test:\n",
    "            return self.sentence, sub_details_list\n",
    "        else:\n",
    "            return self.sentence\n",
    "    \n",
    "    def recursive_beam_search_plainfy(self, beam_width):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fa06fe",
   "metadata": {},
   "source": [
    "#### Example sentences\n",
    "Run each cell below to see output after lexical simplification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ae9af917",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untokenised sentence: A machine learning dream team comprised of Teddy, Daniel, Freddy and Theo.\n",
      "Tokenised sentence: ['a', 'machine', 'learning', 'dream', 'team', 'comprised', 'of', 'teddy', ',', 'daniel', ',', 'freddy', 'and', 'theo', '.']\n",
      "Found NERs: ['teddy', 'daniel', 'freddy', 'theo']\n",
      "Complex probs: [1.52347216e-04 5.97340725e-02 3.08847755e-01 4.28031012e-02\n",
      " 7.29917362e-03 8.85468304e-01 4.54008587e-05 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 6.67599888e-05 0.00000000e+00 0.00000000e+00]\n",
      "Binary complexity labels: [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      "\t Most complex word: comprised \n",
      "\n",
      "Found complex word or expression: ### comprised of ###. Plainifying...\n",
      "Suggested top 5 subtitutions: ['of', 'consisting of', 'composed of', 'that consists of', 'which consists of']\n",
      "\t Simplificaiton complete or no complex expression found.\n",
      "\n",
      "\t Sentence after substitution: a machine learning dream team of teddy , daniel , freddy and theo .\n",
      "\n",
      "Simplification complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a machine learning dream team of teddy , daniel , freddy and theo .'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentence = 'A machine learning dream team comprised of Teddy, Daniel, Freddy and Theo.'\n",
    "sentence = ComplexSentence(input_sentence, label_model=Complexity_labeller_model, tokeniser=tokenizer, verbose=True)\n",
    "sentence.recursive_greedy_plainify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0ca8e3a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untokenised sentence: I took a sip of my drink and kept working.\n",
      "Tokenised sentence: ['i', 'took', 'a', 'sip', 'of', 'my', 'drink', 'and', 'kept', 'working', '.']\n",
      "No NER found.\n",
      "Complex probs: [8.84567271e-05 4.09524131e-04 1.02484584e-04 7.69472897e-01\n",
      " 5.30700163e-05 3.52106465e-04 3.07241362e-02 7.18582814e-05\n",
      " 3.67573537e-02 1.86073082e-03 0.00000000e+00]\n",
      "Binary complexity labels: [0 0 0 1 0 0 0 0 0 0 0]\n",
      "\t Most complex word: sip \n",
      "\n",
      "Found complex word or expression: ### took a sip of ###. Plainifying...\n",
      "Suggested top 5 subtitutions: ['finished', 'took', 'took back', 'took out', 'finished off']\n",
      "\t Simplificaiton complete or no complex expression found.\n",
      "\n",
      "\t Sentence after substitution: i finished my drink and kept working .\n",
      "\n",
      "Simplification complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'i finished my drink and kept working .'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentence = \"I took a sip of my drink and kept working.\"\n",
    "sentence = ComplexSentence(input_sentence, label_model=Complexity_labeller_model, tokeniser=tokenizer, verbose=True)\n",
    "sentence.recursive_greedy_plainify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3da3e902",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untokenised sentence: We will first introduce several fundamental concepts.\n",
      "Tokenised sentence: ['we', 'will', 'first', 'introduce', 'several', 'fundamental', 'concepts', '.']\n",
      "No NER found.\n",
      "Complex probs: [1.09041139e-04 7.78184694e-05 1.84507377e-03 8.66100013e-01\n",
      " 1.30162365e-03 9.16373432e-01 7.82740057e-01 0.00000000e+00]\n",
      "Binary complexity labels: [0 0 0 1 0 1 1 0]\n",
      "\t Most complex word: fundamental \n",
      "\n",
      "Found complex word or expression: ### fundamental ###. Plainifying...\n",
      "Suggested top 5 subtitutions: ['fundamental', 'new', 'basic', 'important', 'key']\n",
      "\t Most complex word: introduce \n",
      "\n",
      "\t Sentence after substitution: we will first introduce several fundamental concepts .\n",
      "\n",
      "Found complex word or expression: ### introduce ###. Plainifying...\n",
      "Suggested top 5 subtitutions: ['introduce', 'establish', 'define', 'discuss', 'present']\n",
      "\t Most complex word: concepts \n",
      "\n",
      "\t Sentence after substitution: we will first introduce several fundamental concepts .\n",
      "\n",
      "Found complex word or expression: ### concepts ###. Plainifying...\n",
      "Suggested top 5 subtitutions: ['concepts', 'principles', 'ideas', 'elements', 'questions']\n",
      "\t Simplificaiton complete or no complex expression found.\n",
      "\n",
      "\t Sentence after substitution: we will first introduce several fundamental concepts .\n",
      "\n",
      "Simplification complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'we will first introduce several fundamental concepts .'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentence = \"We will first introduce several fundamental concepts.\"\n",
    "sentence = ComplexSentence(input_sentence, label_model=Complexity_labeller_model, tokeniser=tokenizer, verbose=True)\n",
    "sentence.recursive_greedy_plainify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e1313ec6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untokenised sentence: Gable also earned an academy award nomination when he portrayed Fletcher Christian in 1935 ' s mutiny on the bounty .\n",
      "Tokenised sentence: ['gable', 'also', 'earned', 'an', 'academy', 'award', 'nomination', 'when', 'he', 'portrayed', 'fletcher', 'christian', 'in', '1935', \"'\", 's', 'mutiny', 'on', 'the', 'bounty', '.']\n",
      "Found NERs: ['gable', 'fletcher', 'christian']\n",
      "Complex probs: [0.00000000e+00 4.39180527e-04 7.13182911e-02 2.01667819e-04\n",
      " 3.79925758e-01 1.26280099e-01 9.61359859e-01 1.41725759e-04\n",
      " 1.29709108e-04 8.90345275e-01 0.00000000e+00 0.00000000e+00\n",
      " 3.56000528e-05 5.31803817e-04 0.00000000e+00 3.95917363e-04\n",
      " 7.23846614e-01 5.05726639e-05 9.24077394e-05 7.14692175e-01\n",
      " 0.00000000e+00]\n",
      "Binary complexity labels: [0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0]\n",
      "\t Most complex word: nomination \n",
      "\n",
      "Found complex word or expression: ### nomination ###. Plainifying...\n",
      "Suggested top 5 subtitutions: ['nomination', 'for acting', 'for sound', 'as well', 'award nomination']\n",
      "\t Most complex word: portrayed \n",
      "\n",
      "\t Sentence after substitution: gable also earned an academy award nomination when he portrayed fletcher christian in 1935 ' s mutiny on the bounty .\n",
      "\n",
      "Found complex word or expression: ### portrayed ###. Plainifying...\n",
      "Suggested top 5 subtitutions: ['portrayed', 'played', 'portrayed captain', 'appeared as', 'played captain']\n",
      "\t Most complex word: mutiny \n",
      "\n",
      "\t Sentence after substitution: gable also earned an academy award nomination when he portrayed fletcher christian in 1935 ' s mutiny on the bounty .\n",
      "\n",
      "Found complex word or expression: ### mutiny ###. Plainifying...\n",
      "Suggested top 5 subtitutions: ['mutiny', 'film mutiny', 'the man', 'man', 'traitors']\n",
      "\t Most complex word: bounty \n",
      "\n",
      "\t Sentence after substitution: gable also earned an academy award nomination when he portrayed fletcher christian in 1935 ' s mutiny on the bounty .\n",
      "\n",
      "Found complex word or expression: ### the bounty ###. Plainifying...\n",
      "Suggested top 5 subtitutions: ['the bounty', 'the', 'a', 'board', 'bounty']\n",
      "\t Simplificaiton complete or no complex expression found.\n",
      "\n",
      "\t Sentence after substitution: gable also earned an academy award nomination when he portrayed fletcher christian in 1935 ' s mutiny on the bounty .\n",
      "\n",
      "Simplification complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"gable also earned an academy award nomination when he portrayed fletcher christian in 1935 ' s mutiny on the bounty .\""
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentence = \"Gable also earned an academy award nomination when he portrayed Fletcher Christian in 1935 ' s mutiny on the bounty .\"\n",
    "sentence = ComplexSentence(input_sentence, label_model=Complexity_labeller_model, tokeniser=tokenizer, verbose=True)\n",
    "sentence.recursive_greedy_plainify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3f8be878",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untokenised sentence: Probability is the branch of mathematics concerning numerical descriptions of how likely an event is to occur, or how likely it is that a proposition is true.\n",
      "Tokenised sentence: ['probability', 'is', 'the', 'branch', 'of', 'mathematics', 'concerning', 'numerical', 'descriptions', 'of', 'how', 'likely', 'an', 'event', 'is', 'to', 'occur', ',', 'or', 'how', 'likely', 'it', 'is', 'that', 'a', 'proposition', 'is', 'true', '.']\n",
      "No NER found.\n",
      "Complex probs: [8.30549002e-01 6.26499386e-05 1.10454857e-04 5.26121318e-01\n",
      " 6.59589932e-05 8.48085880e-01 8.09274793e-01 6.57362401e-01\n",
      " 9.07784224e-01 4.00023018e-05 1.80853662e-04 9.50911082e-03\n",
      " 1.38362753e-04 1.05769299e-01 5.40117035e-05 5.99568411e-05\n",
      " 2.35088870e-01 0.00000000e+00 8.21707072e-05 3.28921247e-04\n",
      " 1.09109739e-02 1.23351288e-04 5.08484009e-05 8.06485332e-05\n",
      " 1.08139444e-04 8.89525056e-01 5.58543907e-05 1.00519070e-02\n",
      " 0.00000000e+00]\n",
      "Binary complexity labels: [1 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "\t Most complex word: descriptions \n",
      "\n",
      "Found complex word or expression: ### descriptions of ###. Plainifying...\n",
      "Suggested top 5 subtitutions: ['measures of', 'estimates of', 'determination of', 'expression of', 'values of']\n",
      "\t Most complex word: probability \n",
      "\n",
      "\t Sentence after substitution: probability is the branch of mathematics concerning numerical measures of how likely an event is to occur , or how likely it is that a proposition is true .\n",
      "\n",
      "Found complex word or expression: ### probability ###. Plainifying...\n",
      "Suggested top 5 subtitutions: ['probability theory', 'probability', '. probability', 'or probability', 'theory']\n",
      "\t Most complex word: proposition \n",
      "\n",
      "\t Sentence after substitution: probability theory is the branch of mathematics concerning numerical measures of how likely an event is to occur , or how likely it is that a proposition is true .\n",
      "\n",
      "Found complex word or expression: ### a proposition ###. Plainifying...\n",
      "Suggested top 5 subtitutions: ['it', 'a statement', 'something', 'a claim', 'an event']\n",
      "\t Most complex word: mathematics \n",
      "\n",
      "\t Sentence after substitution: probability theory is the branch of mathematics concerning numerical measures of how likely an event is to occur , or how likely it is that it is true .\n",
      "\n",
      "Found complex word or expression: ### of mathematics ###. Plainifying...\n",
      "Suggested top 5 subtitutions: ['of mathematics', 'of probability theory', 'of', 'of probability', 'of science']\n",
      "\t Most complex word: concerning \n",
      "\n",
      "\t Sentence after substitution: probability theory is the branch of mathematics concerning numerical measures of how likely an event is to occur , or how likely it is that it is true .\n",
      "\n",
      "Found complex word or expression: ### concerning ###. Plainifying...\n",
      "Suggested top 5 subtitutions: ['concerning', 'with', 'involving', 'concerned with', 'that studies']\n",
      "\t Most complex word: numerical \n",
      "\n",
      "\t Sentence after substitution: probability theory is the branch of mathematics concerning numerical measures of how likely an event is to occur , or how likely it is that it is true .\n",
      "\n",
      "Found complex word or expression: ### numerical ###. Plainifying...\n",
      "Suggested top 5 subtitutions: ['the', 'various', 'numerical', 'measures or', 'probability']\n",
      "\t Most complex word: branch \n",
      "\n",
      "\t Sentence after substitution: probability theory is the branch of mathematics concerning the measures of how likely an event is to occur , or how likely it is that it is true .\n",
      "\n",
      "Found complex word or expression: ### the branch of ###. Plainifying...\n",
      "Suggested top 5 subtitutions: ['the', 'part of', 'a branch of', 'a', 'branch of']\n",
      "\t Simplificaiton complete or no complex expression found.\n",
      "\n",
      "\t Sentence after substitution: probability theory is the mathematics concerning the measures of how likely an event is to occur , or how likely it is that it is true .\n",
      "\n",
      "Simplification complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'probability theory is the mathematics concerning the measures of how likely an event is to occur , or how likely it is that it is true .'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentence = \"Probability is the branch of mathematics concerning numerical descriptions of how likely an event is to occur, or how likely it is that a proposition is true.\"\n",
    "sentence = ComplexSentence(input_sentence, label_model=Complexity_labeller_model, tokeniser=tokenizer, verbose=True)\n",
    "sentence.recursive_greedy_plainify()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269d7839",
   "metadata": {},
   "source": [
    "### Tests\n",
    "Run any tests below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ca3359f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search alpha\n",
    "alphas = []\n",
    "for a1 in range(10):\n",
    "    for a2 in range(10):\n",
    "        alphas.append((a1/9, a2/9, (9-a1-a2)/9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7ea1097c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\"one side of the armed conflicts is composed mainly of the sudanese military and the Janjaweed , a sudanese militia group recruited mostly from the afro - arab Abbala tribes of the northern Rizeigat region in Sudan .\",\n",
    "\"Jeddah is the principal gateway to Mecca , Islam ' s holiest city , which able - bodied muslims are required to visit at least once in their lifetime .\",\n",
    "\"the great dark spot is thought to represent a hole in the methane cloud deck of Neptune .\",\n",
    "\"his next work , saturday , follows an especially eventful day in the life of a successful neurosurgeon .\",\n",
    "\"the tarantula , the trickster character , spun a black cord and , attaching it to the ball , crawled away fast to the east , pulling on the cord with all his strength .\",\n",
    "\"there he died six weeks later , on 13 january 888 \",\n",
    "\"they are culturally akin to the coastal peoples of Papua New Guinea .\",\n",
    "\"since 2000 , the recipient of the Kate Greenaway Medal has also been presented with the Colin Mears Award to the value of £ 5000 .\",\n",
    "\"following the drummers are dancers , who often play the sogo  (  a tiny drum that makes almost no sound  )  and tend to have more elaborate — even acrobatic — choreography .\",\n",
    "\"the spacecraft consists of two main elements : the Nasa Cassini Orbiter , named after the italian - french astronomer Giovanni Domenico Cassini , and the Esa Huygens Probe , named after the dutch astronomer , mathematician and physicist Christiaan Huygens .\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d8d6a6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 0.0, 1.0)\n",
      "(0.0, 0.1111111111111111, 0.8888888888888888)\n",
      "(0.0, 0.2222222222222222, 0.7777777777777778)\n",
      "(0.0, 0.3333333333333333, 0.6666666666666666)\n",
      "(0.0, 0.4444444444444444, 0.5555555555555556)\n",
      "(0.0, 0.5555555555555556, 0.4444444444444444)\n",
      "(0.0, 0.6666666666666666, 0.3333333333333333)\n",
      "(0.0, 0.7777777777777778, 0.2222222222222222)\n",
      "(0.0, 0.8888888888888888, 0.1111111111111111)\n",
      "(0.0, 1.0, 0.0)\n",
      "(0.1111111111111111, 0.0, 0.8888888888888888)\n",
      "(0.1111111111111111, 0.1111111111111111, 0.7777777777777778)\n",
      "(0.1111111111111111, 0.2222222222222222, 0.6666666666666666)\n",
      "(0.1111111111111111, 0.3333333333333333, 0.5555555555555556)\n",
      "(0.1111111111111111, 0.4444444444444444, 0.4444444444444444)\n",
      "(0.1111111111111111, 0.5555555555555556, 0.3333333333333333)\n",
      "(0.1111111111111111, 0.6666666666666666, 0.2222222222222222)\n",
      "(0.1111111111111111, 0.7777777777777778, 0.1111111111111111)\n",
      "(0.1111111111111111, 0.8888888888888888, 0.0)\n",
      "(0.1111111111111111, 1.0, -0.1111111111111111)\n",
      "(0.2222222222222222, 0.0, 0.7777777777777778)\n",
      "(0.2222222222222222, 0.1111111111111111, 0.6666666666666666)\n",
      "(0.2222222222222222, 0.2222222222222222, 0.5555555555555556)\n",
      "(0.2222222222222222, 0.3333333333333333, 0.4444444444444444)\n",
      "(0.2222222222222222, 0.4444444444444444, 0.3333333333333333)\n",
      "(0.2222222222222222, 0.5555555555555556, 0.2222222222222222)\n",
      "(0.2222222222222222, 0.6666666666666666, 0.1111111111111111)\n",
      "(0.2222222222222222, 0.7777777777777778, 0.0)\n",
      "(0.2222222222222222, 0.8888888888888888, -0.1111111111111111)\n",
      "(0.2222222222222222, 1.0, -0.2222222222222222)\n",
      "(0.3333333333333333, 0.0, 0.6666666666666666)\n",
      "(0.3333333333333333, 0.1111111111111111, 0.5555555555555556)\n",
      "(0.3333333333333333, 0.2222222222222222, 0.4444444444444444)\n",
      "(0.3333333333333333, 0.3333333333333333, 0.3333333333333333)\n",
      "(0.3333333333333333, 0.4444444444444444, 0.2222222222222222)\n",
      "(0.3333333333333333, 0.5555555555555556, 0.1111111111111111)\n",
      "(0.3333333333333333, 0.6666666666666666, 0.0)\n",
      "(0.3333333333333333, 0.7777777777777778, -0.1111111111111111)\n",
      "(0.3333333333333333, 0.8888888888888888, -0.2222222222222222)\n",
      "(0.3333333333333333, 1.0, -0.3333333333333333)\n",
      "(0.4444444444444444, 0.0, 0.5555555555555556)\n",
      "(0.4444444444444444, 0.1111111111111111, 0.4444444444444444)\n",
      "(0.4444444444444444, 0.2222222222222222, 0.3333333333333333)\n",
      "(0.4444444444444444, 0.3333333333333333, 0.2222222222222222)\n",
      "(0.4444444444444444, 0.4444444444444444, 0.1111111111111111)\n",
      "(0.4444444444444444, 0.5555555555555556, 0.0)\n",
      "(0.4444444444444444, 0.6666666666666666, -0.1111111111111111)\n",
      "(0.4444444444444444, 0.7777777777777778, -0.2222222222222222)\n",
      "(0.4444444444444444, 0.8888888888888888, -0.3333333333333333)\n",
      "(0.4444444444444444, 1.0, -0.4444444444444444)\n",
      "(0.5555555555555556, 0.0, 0.4444444444444444)\n",
      "(0.5555555555555556, 0.1111111111111111, 0.3333333333333333)\n",
      "(0.5555555555555556, 0.2222222222222222, 0.2222222222222222)\n",
      "(0.5555555555555556, 0.3333333333333333, 0.1111111111111111)\n",
      "(0.5555555555555556, 0.4444444444444444, 0.0)\n",
      "(0.5555555555555556, 0.5555555555555556, -0.1111111111111111)\n",
      "(0.5555555555555556, 0.6666666666666666, -0.2222222222222222)\n",
      "(0.5555555555555556, 0.7777777777777778, -0.3333333333333333)\n",
      "(0.5555555555555556, 0.8888888888888888, -0.4444444444444444)\n",
      "(0.5555555555555556, 1.0, -0.5555555555555556)\n",
      "(0.6666666666666666, 0.0, 0.3333333333333333)\n",
      "(0.6666666666666666, 0.1111111111111111, 0.2222222222222222)\n",
      "(0.6666666666666666, 0.2222222222222222, 0.1111111111111111)\n",
      "(0.6666666666666666, 0.3333333333333333, 0.0)\n",
      "(0.6666666666666666, 0.4444444444444444, -0.1111111111111111)\n",
      "(0.6666666666666666, 0.5555555555555556, -0.2222222222222222)\n",
      "(0.6666666666666666, 0.6666666666666666, -0.3333333333333333)\n",
      "(0.6666666666666666, 0.7777777777777778, -0.4444444444444444)\n",
      "(0.6666666666666666, 0.8888888888888888, -0.5555555555555556)\n",
      "(0.6666666666666666, 1.0, -0.6666666666666666)\n",
      "(0.7777777777777778, 0.0, 0.2222222222222222)\n",
      "(0.7777777777777778, 0.1111111111111111, 0.1111111111111111)\n",
      "(0.7777777777777778, 0.2222222222222222, 0.0)\n",
      "(0.7777777777777778, 0.3333333333333333, -0.1111111111111111)\n",
      "(0.7777777777777778, 0.4444444444444444, -0.2222222222222222)\n",
      "(0.7777777777777778, 0.5555555555555556, -0.3333333333333333)\n",
      "(0.7777777777777778, 0.6666666666666666, -0.4444444444444444)\n",
      "(0.7777777777777778, 0.7777777777777778, -0.5555555555555556)\n",
      "(0.7777777777777778, 0.8888888888888888, -0.6666666666666666)\n",
      "(0.7777777777777778, 1.0, -0.7777777777777778)\n",
      "(0.8888888888888888, 0.0, 0.1111111111111111)\n",
      "(0.8888888888888888, 0.1111111111111111, 0.0)\n",
      "(0.8888888888888888, 0.2222222222222222, -0.1111111111111111)\n",
      "(0.8888888888888888, 0.3333333333333333, -0.2222222222222222)\n",
      "(0.8888888888888888, 0.4444444444444444, -0.3333333333333333)\n",
      "(0.8888888888888888, 0.5555555555555556, -0.4444444444444444)\n",
      "(0.8888888888888888, 0.6666666666666666, -0.5555555555555556)\n",
      "(0.8888888888888888, 0.7777777777777778, -0.6666666666666666)\n",
      "(0.8888888888888888, 0.8888888888888888, -0.7777777777777778)\n",
      "(0.8888888888888888, 1.0, -0.8888888888888888)\n",
      "(1.0, 0.0, 0.0)\n",
      "(1.0, 0.1111111111111111, -0.1111111111111111)\n",
      "(1.0, 0.2222222222222222, -0.2222222222222222)\n",
      "(1.0, 0.3333333333333333, -0.3333333333333333)\n",
      "(1.0, 0.4444444444444444, -0.4444444444444444)\n",
      "(1.0, 0.5555555555555556, -0.5555555555555556)\n",
      "(1.0, 0.6666666666666666, -0.6666666666666666)\n",
      "(1.0, 0.7777777777777778, -0.7777777777777778)\n",
      "(1.0, 0.8888888888888888, -0.8888888888888888)\n",
      "(1.0, 1.0, -1.0)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "for alpha in alphas:\n",
    "    \n",
    "    final_sentence = []\n",
    "    sentence_details_list = []\n",
    "    \n",
    "    for input_sentence in test_sentences:\n",
    "    \n",
    "        input_sentence = norm_test_dat[i]\n",
    "        s = ComplexSentence(input_sentence, label_model=Complexity_labeller_model, tokeniser=tokenizer, verbose=True)\n",
    "        sentence_final, sentence_detail = s.recursive_greedy_plainify()\n",
    "        sentence.append(sentence_final)\n",
    "        sentence_details = {'sentence_id': i, 'details': sentence_detail}\n",
    "        sentence_details_list.append(sentence_details)\n",
    "    \n",
    "    with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d98fead",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
