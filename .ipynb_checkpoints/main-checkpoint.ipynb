{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c293036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/daniel/.local/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "from CWIs.complex_labeller import Complexity_labeller\n",
    "from plainifier.plainify import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9d16d90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/daniel/Documents/MSc_ML_UCL/COMP0087/LSBert_new/CWIs/labeler.py:152: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/daniel/.local/lib/python3.9/site-packages/tensorflow/python/ops/rnn.py:437: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/daniel/.local/lib/python3.9/site-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:992: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/daniel/.local/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1082: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/Documents/MSc_ML_UCL/COMP0087/LSBert_new/CWIs/labeler.py:141: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  char_lstm_cell_fw = tf.nn.rnn_cell.LSTMCell(self.config[\"char_recurrent_size\"],\n",
      "/home/daniel/Documents/MSc_ML_UCL/COMP0087/LSBert_new/CWIs/labeler.py:146: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  char_lstm_cell_bw = tf.nn.rnn_cell.LSTMCell(self.config[\"char_recurrent_size\"],\n",
      "/home/daniel/.local/lib/python3.9/site-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:984: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  self._kernel = self.add_variable(\n",
      "/home/daniel/.local/lib/python3.9/site-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:993: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  self._bias = self.add_variable(\n",
      "/home/daniel/Documents/MSc_ML_UCL/COMP0087/LSBert_new/CWIs/labeler.py:165: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  char_output_tensor = tf.layers.dense(char_output_tensor, char_hidden_layer_size, activation=tf.tanh, kernel_initializer=self.initializer)\n",
      "/home/daniel/.local/lib/python3.9/site-packages/keras/legacy_tf_layers/core.py:261: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  return layer.apply(inputs)\n",
      "/home/daniel/Documents/MSc_ML_UCL/COMP0087/LSBert_new/CWIs/labeler.py:182: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  attention_output = tf.layers.dense(attention_evidence_tensor, self.config[\"word_embedding_size\"], activation=tf.tanh, kernel_initializer=self.initializer)\n",
      "/home/daniel/Documents/MSc_ML_UCL/COMP0087/LSBert_new/CWIs/labeler.py:183: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  attention_output = tf.layers.dense(attention_output, self.config[\"word_embedding_size\"], activation=tf.sigmoid, kernel_initializer=self.initializer)\n",
      "/home/daniel/Documents/MSc_ML_UCL/COMP0087/LSBert_new/CWIs/labeler.py:193: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  word_lstm_cell_fw = tf.nn.rnn_cell.LSTMCell(self.config[\"word_recurrent_size\"],\n",
      "/home/daniel/Documents/MSc_ML_UCL/COMP0087/LSBert_new/CWIs/labeler.py:198: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  word_lstm_cell_bw = tf.nn.rnn_cell.LSTMCell(self.config[\"word_recurrent_size\"],\n",
      "/home/daniel/Documents/MSc_ML_UCL/COMP0087/LSBert_new/CWIs/labeler.py:263: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  lmcost_hidden_layer = tf.layers.dense(input_tensor, self.config[\"lmcost_hidden_layer_size\"], activation=tf.tanh, kernel_initializer=self.initializer)\n",
      "/home/daniel/Documents/MSc_ML_UCL/COMP0087/LSBert_new/CWIs/labeler.py:264: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  lmcost_output = tf.layers.dense(lmcost_hidden_layer, lmcost_max_vocab_size, activation=None, kernel_initializer=self.initializer)\n",
      "/home/daniel/Documents/MSc_ML_UCL/COMP0087/LSBert_new/CWIs/labeler.py:220: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  processed_tensor = tf.layers.dense(processed_tensor, self.config[\"hidden_layer_size\"], activation=tf.tanh, kernel_initializer=self.initializer)\n",
      "/home/daniel/Documents/MSc_ML_UCL/COMP0087/LSBert_new/CWIs/labeler.py:223: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  self.scores = tf.layers.dense(processed_tensor, len(self.label2id), activation=None, kernel_initializer=self.initializer, name=\"output_ff\")\n",
      "2022-03-31 22:32:13.920663: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-31 22:32:13.924249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-31 22:32:13.924663: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-03-31 22:32:13.924673: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "two_gram_mwes_list = './CWIs/2_gram_mwe_50.txt'\n",
    "three_gram_mwes_list = './CWIs/3_gram_mwe_25.txt'\n",
    "four_gram_mwes_list = './CWIs/4_gram_mwe_8.txt'\n",
    "pretrained_model_path = './CWIs/cwi_seq.model'\n",
    "temp_path = './CWIs/temp_file.txt'\n",
    "\n",
    "path = './plainifier/'\n",
    "premodel = 'bert-large-uncased-whole-word-masking'\n",
    "bert_dict = 'tersebert_pytorch_1_0.bin'\n",
    "embedding = 'crawl-300d-2M-subword.vec'\n",
    "unigram = 'unigrams-df.tsv'\n",
    "tokenizer = BertTokenizer.from_pretrained(premodel)\n",
    "Complexity_labeller_model = Complexity_labeller(pretrained_model_path, temp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19f06e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 2000000/2000000 [01:29<00:00, 22353.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Embeddings\n",
      "Loading Unigrams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 8394369/8394369 [05:52<00:00, 23788.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Unigrams\n"
     ]
    }
   ],
   "source": [
    "# loading bert model, word embeddings and unigrams. This process takes 7 minutes\n",
    "model, similm, tokenfreq, embeddings, vocabulary2 = load_all(path, premodel, bert_dict, embedding, unigram, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5d229d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexSentence:\n",
    "    # Sentence class\n",
    "    def __init__(self, sentence, label_model, tokeniser, verbose=True, beam_width=3):\n",
    "        self.sentence = sentence\n",
    "        self.tokenised_sentence = tokeniser.tokenize(self.sentence)\n",
    "        self.label_model = label_model\n",
    "        self.verbose = verbose\n",
    "        self.beam_width = beam_width\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f'Untokenised sentence: {self.sentence}')\n",
    "            print(f'Tokenised sentence: {self.tokenised_sentence}')\n",
    "\n",
    "        self.label_complex_words()\n",
    "\n",
    "    def label_complex_words(self, init=True):\n",
    "        # applying complexity labeller to the sentence\n",
    "\n",
    "        Complexity_labeller.convert_format_string(self.label_model, self.sentence)\n",
    "        if init:\n",
    "            self.bin_labels = Complexity_labeller.get_bin_labels(self.label_model)[0]\n",
    "        self.is_complex = True if np.sum(self.bin_labels) >= 1 else False\n",
    "        self.probs = Complexity_labeller.get_prob_labels(self.label_model)\n",
    "        \n",
    "        self.complexity_ranking = np.argsort(np.array(self.bin_labels) * np.array(self.probs))[::-1]\n",
    "        self.most_complex_word = self.tokenised_sentence[self.complexity_ranking[0]]\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f'Complex probs: {self.probs}')\n",
    "            print(f'Binary complexity labels: {self.bin_labels}')\n",
    "            \n",
    "            if self.is_complex:\n",
    "                print(f'\\t Most complex word: {self.most_complex_word} \\n')\n",
    "        \n",
    "        if not self.is_complex:\n",
    "            print(f'\\t Simplificaiton complete or no complex expression found.\\n')\n",
    "    \n",
    "    def find_MWEs_w_most_complex_word(self, n_gram, filepath):\n",
    "        # finds the n-gram mwe of the most complex word in the sentence, if any\n",
    "        # returns: mwe positions or complex word positions\n",
    "        \n",
    "        complex_word_pos = self.complexity_ranking[0]\n",
    "\n",
    "        if complex_word_pos - n_gram + 1 > 0:\n",
    "            sliding_start = complex_word_pos - n_gram + 1\n",
    "        else:\n",
    "            sliding_start = 0\n",
    "        \n",
    "        if complex_word_pos + n_gram - 1 < len(self.complexity_ranking):\n",
    "            sliding_end = complex_word_pos\n",
    "        else:\n",
    "            sliding_end = len(self.complexity_ranking) - n_gram\n",
    "\n",
    "        with open(filepath, 'r') as f:\n",
    "            mwes = set(f.read().split('\\n')) # make set\n",
    "            avg_mwe_complexity = 0\n",
    "            for pos in range(sliding_start, sliding_end + 1):\n",
    "                possible_mwe = ' '.join(self.tokenised_sentence[pos: pos + n_gram])\n",
    "                \n",
    "                if possible_mwe in mwes:\n",
    "                    \n",
    "                    if np.mean(self.probs[pos:pos+n_gram]) > avg_mwe_complexity:\n",
    "                        avg_mwe_complexity = np.mean(self.probs[pos:pos+n_gram])\n",
    "                        valid_mwes_idx = np.arange(pos, pos+n_gram, 1)\n",
    "                        mwe_found = possible_mwe\n",
    "                    else:\n",
    "                        continue\n",
    "                        \n",
    "        if avg_mwe_complexity > 0:\n",
    "            self.idx_to_plainify = valid_mwes_idx\n",
    "        else:\n",
    "            self.idx_to_plainify = [complex_word_pos]\n",
    "        \n",
    "    \n",
    "    def find_all_ngram_mwes(self):\n",
    "        # returns: self.idx_to_plainify the indices of the longest mwe found\n",
    "        \n",
    "        if not self.is_complex:\n",
    "            raise ValueError('Sentence is not complex')\n",
    "        \n",
    "        # give priority to longer MWEs\n",
    "        n_gram_files = {2: two_gram_mwes_list, 3: three_gram_mwes_list, 4:four_gram_mwes_list}\n",
    "        \n",
    "        for n in reversed(range(2,5)):\n",
    "            self.find_MWEs_w_most_complex_word(n, n_gram_files[n])\n",
    "            \n",
    "            if len(self.idx_to_plainify) == n: # if such mwe is found\n",
    "                break\n",
    "    \n",
    "    def one_step_plainify(self):\n",
    "        idx_start = self.idx_to_plainify[0]\n",
    "        idx_end = self.idx_to_plainify[-1]+1\n",
    "        print(f'Found complex word or expression: ### {\" \".join(self.tokenised_sentence[idx_start:idx_end])} ###. Plainifying...')\n",
    "        processed_sentence = tokeniseUntokenise(self.sentence, tokenizer)\n",
    "        forward_result = getTokenReplacement(processed_sentence, idx_start, len(self.idx_to_plainify), \n",
    "                                  tokenizer, model, similm, tokenfreq, embeddings, vocabulary2,\n",
    "                                  verbose=False, backwards=False, maxDepth=3, maxBreadth=16, alpha=(1/9,6/9,2/9))\n",
    "        backward_result = getTokenReplacement(processed_sentence, idx_start, len(self.idx_to_plainify),\n",
    "                                  tokenizer, model, similm, tokenfreq, embeddings, vocabulary2, \n",
    "                                  verbose=False, backwards=True, maxDepth=3, maxBreadth=16, alpha=(1/9,6/9,2/9))\n",
    "        words, scores = aggregateResults((forward_result, backward_result))\n",
    "        print(f'Suggested top 5 subtitutions: {words[:5]}')\n",
    "        return words[0].split(' ')\n",
    "        \n",
    "    \n",
    "    def sub_in_sentence(self, substitution):\n",
    "        # plugs a substitution in the sentence, then updates complexity scores\n",
    "        substitution_len = len(substitution)\n",
    "        \n",
    "        idx_start = self.idx_to_plainify[0]\n",
    "        idx_end = self.idx_to_plainify[-1]+1\n",
    "        \n",
    "        self.tokenised_sentence = self.tokenised_sentence[:idx_start] + substitution + self.tokenised_sentence[idx_end:]\n",
    "        self.sentence = ' '.join(self.tokenised_sentence)\n",
    "        self.bin_labels = list(self.bin_labels[:idx_start]) + [0] * substitution_len + list(self.bin_labels[idx_end:])\n",
    "        self.label_complex_words(init=False)\n",
    "        print(f'\\n\\t Sentence after substitution: {self.sentence}\\n')\n",
    "        \n",
    "    def recursive_greedy_plainify(self, max_steps=float('inf')):\n",
    "        n = 1\n",
    "        while self.is_complex and n < max_steps:\n",
    "            self.find_all_ngram_mwes()\n",
    "            sub = self.one_step_plainify()\n",
    "            self.sub_in_sentence(sub)\n",
    "            n += 1\n",
    "        print(f'Simplification complete.')\n",
    "    \n",
    "    def recursive_beam_search_plainfy(self, beam_width):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c304e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence class\n",
    "    def __init__(self, sentence, label_model, tokeniser, verbose=True):\n",
    "        self.sentence = sentence\n",
    "        #self.tokenised_sentence = tokeniser.tokenize(self.sentence)\n",
    "        self.tokenised_sentence = self.generate_tokenised_sentence()\n",
    "        self.label_model = label_model\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f'Untokenised sentence: {self.sentence}')\n",
    "            print(f'Tokenised sentence: {self.tokenised_sentence}')\n",
    "\n",
    "        self.label_complex_words()\n",
    "        #print(self.generate_tokenised_sentence())\n",
    "        \n",
    "    def generate_tokenised_sentence(self):\n",
    "        tokens = tokeniseUntokenise(self.sentence, tokenizer)['tokens']\n",
    "        word_idx = tokeniseUntokenise(self.sentence, tokenizer)['words']\n",
    "        tokenised_sentence_list = []\n",
    "        for idx_list in word_idx:\n",
    "            if len(idx_list)==1:\n",
    "                tokenised_sentence_list.append(np.array(tokens)[idx_list[0]])\n",
    "            else:\n",
    "                word_untokenised = ''\n",
    "                for idx_list_untokenised in idx_list:\n",
    "                    word_untokenised += np.array(tokens)[idx_list_untokenised].replace('##', '')\n",
    "                tokenised_sentence_list.append(word_untokenised)\n",
    "        return tokenised_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "476bcdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found complex word or expression: ### descriptions of ###. Plainifying...\n",
      "Suggested top 5 subtitutions: ['measures of', 'estimates of', 'determination of', 'expression of', 'values of']\n",
      "\n",
      "\t Sentence after substitution: probability is the branch of mathematics concerning numerical measures of how likely an event is to occur , or how likely it is that a proposition is true .\n",
      "\n",
      "Found complex word or expression: ### probability ###. Plainifying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/Documents/MSc_ML_UCL/COMP0087/LSBert_new/plainifier/plainify.py:235: RuntimeWarning: invalid value encountered in power\n",
      "  finalGoodness=(scoresVec**alpha[0])*(similVec**alpha[1])*(freqVec**alpha[2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggested top 5 subtitutions: ['probability theory', 'probability', '. probability', 'or probability', 'theory']\n",
      "\n",
      "\t Sentence after substitution: probability theory is the branch of mathematics concerning numerical measures of how likely an event is to occur , or how likely it is that a proposition is true .\n",
      "\n",
      "Found complex word or expression: ### a proposition ###. Plainifying...\n",
      "Suggested top 5 subtitutions: ['it', 'a statement', 'something', 'a claim', 'an event']\n",
      "\n",
      "\t Sentence after substitution: probability theory is the branch of mathematics concerning numerical measures of how likely an event is to occur , or how likely it is that it is true .\n",
      "\n",
      "Found complex word or expression: ### of mathematics ###. Plainifying...\n",
      "Suggested top 5 subtitutions: ['of mathematics', 'of probability theory', 'of', 'of probability', 'of science']\n",
      "\n",
      "\t Sentence after substitution: probability theory is the branch of mathematics concerning numerical measures of how likely an event is to occur , or how likely it is that it is true .\n",
      "\n",
      "Found complex word or expression: ### concerning ###. Plainifying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/Documents/MSc_ML_UCL/COMP0087/LSBert_new/plainifier/plainify.py:235: RuntimeWarning: invalid value encountered in power\n",
      "  finalGoodness=(scoresVec**alpha[0])*(similVec**alpha[1])*(freqVec**alpha[2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggested top 5 subtitutions: ['concerning', 'with', 'involving', 'concerned with', 'that studies']\n",
      "\n",
      "\t Sentence after substitution: probability theory is the branch of mathematics concerning numerical measures of how likely an event is to occur , or how likely it is that it is true .\n",
      "\n",
      "Found complex word or expression: ### numerical ###. Plainifying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/Documents/MSc_ML_UCL/COMP0087/LSBert_new/plainifier/plainify.py:235: RuntimeWarning: invalid value encountered in power\n",
      "  finalGoodness=(scoresVec**alpha[0])*(similVec**alpha[1])*(freqVec**alpha[2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggested top 5 subtitutions: ['the', 'various', 'numerical', 'measures or', 'probability']\n",
      "\n",
      "\t Sentence after substitution: probability theory is the branch of mathematics concerning the measures of how likely an event is to occur , or how likely it is that it is true .\n",
      "\n",
      "Found complex word or expression: ### the branch of ###. Plainifying...\n",
      "Suggested top 5 subtitutions: ['the', 'part of', 'a branch of', 'a', 'branch of']\n",
      "\t Simplificaiton complete or no complex expression found.\n",
      "\n",
      "\n",
      "\t Sentence after substitution: probability theory is the mathematics concerning the measures of how likely an event is to occur , or how likely it is that it is true .\n",
      "\n",
      "Simplification complete.\n"
     ]
    }
   ],
   "source": [
    "input_sentence = \"Probability is the branch of mathematics concerning numerical descriptions of how likely an event is to occur, or how likely it is that a proposition is true.\"\n",
    "\n",
    "s = ComplexSentence(input_sentence, label_model=Complexity_labeller_model, tokeniser=tokenizer, verbose=False)\n",
    "s.recursive_greedy_plainify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ca8e3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Found complex word or expression: blazing. \n",
      " \t Plainifying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/Documents/MSc_ML_UCL/COMP0087/LSBert_new/plainifier/plainify.py:235: RuntimeWarning: invalid value encountered in power\n",
      "  finalGoodness=(scoresVec**alpha[0])*(similVec**alpha[1])*(freqVec**alpha[2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggested top 5 subtitutions: ['deep', 'clear', 'bright', 'dark', 'light']\n"
     ]
    }
   ],
   "source": [
    "input_sentence = \"I took a sip of coffee and kept working.\"\n",
    "\n",
    "s = ComplexSentence(input_sentence, label_model=Complexity_labeller_model, tokeniser=tokenizer, verbose=False)\n",
    "\n",
    "s.find_all_ngram_mwes()\n",
    "\n",
    "s.one_step_plainify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3da3e902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Found complex word or expression: fundamental. \n",
      " \t Plainifying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/Documents/MSc_ML_UCL/COMP0087/LSBert_new/plainifier/plainify.py:235: RuntimeWarning: invalid value encountered in power\n",
      "  finalGoodness=(scoresVec**alpha[0])*(similVec**alpha[1])*(freqVec**alpha[2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggested top 5 subtitutions: ['fundamental', 'new', 'basic', 'important', 'main']\n"
     ]
    }
   ],
   "source": [
    "input_sentence = \"We will first introduce several fundamental concepts\"\n",
    "\n",
    "s = ComplexSentence(input_sentence, label_model=Complexity_labeller_model, tokeniser=tokenizer, verbose=False)\n",
    "\n",
    "s.find_MWEs_w_most_complex_word(n_gram=3, filepath=three_gram_mwes_list)\n",
    "\n",
    "s.one_step_plainify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "59608142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Found complex word or expression: patterns in. \n",
      " \t Plainifying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/Documents/MSc_ML_UCL/COMP0087/LSBert_new/plainifier/plainify.py:235: RuntimeWarning: invalid value encountered in power\n",
      "  finalGoodness=(scoresVec**alpha[0])*(similVec**alpha[1])*(freqVec**alpha[2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggested top 5 subtitutions: ['and process', 'and understand', 'patterns in', 'and interpret', 'or process']\n"
     ]
    }
   ],
   "source": [
    "input_sentence = \"A neural network is a series of rules that attempt to recognize patterns in a set of data through a process that is the way the human brain operates.\"\n",
    "\n",
    "s = ComplexSentence(input_sentence, label_model=Complexity_labeller_model, tokeniser=tokenizer, verbose=False)\n",
    "s.find_all_ngram_mwes()\n",
    "s.one_step_plainify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be31f532",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
